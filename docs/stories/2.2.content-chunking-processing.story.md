# Story 2.2: Content Chunking & Processing

## Status
Draft

## Story
**As a** RAG pipeline system,
**I want** a content chunking service that creates optimized text chunks with metadata from ingested textbooks,
**so that** I can prepare structured content for vector embedding and efficient retrieval.

## Acceptance Criteria
1. Basic chunking strategy implemented using RecursiveCharacterTextSplitter with configurable parameters
2. Chunk metadata generation with unique hash-based chunk_id and source traceability  
3. Language detection for English/Thai content routing and processing optimization
4. Content preprocessing pipeline for text cleaning, normalization, and quality validation
5. Chunk storage in PostgreSQL chunks table with proper indexing and relationships
6. Integration with async processing using Celery for large document handling
7. Performance optimization for processing standard Physics textbooks efficiently
8. Integration tests validate chunking quality, metadata accuracy, and storage operations

## Tasks / Subtasks
- [ ] Implement basic chunking strategy (AC: 1)
  - [ ] Setup RecursiveCharacterTextSplitter with Physics content optimization
  - [ ] Configure chunk size (1000 chars) and overlap (200 chars) parameters
  - [ ] Add chunk boundary detection to avoid splitting mid-sentence
  - [ ] Implement configurable chunking parameters via YAML config
  - [ ] Add chunk size validation and automatic adjustment

- [ ] Create chunk metadata system (AC: 2)
  - [ ] Generate unique hash-based chunk_id for each text chunk
  - [ ] Add source traceability (textbook_id, page_number, section)
  - [ ] Include chunk position metadata (sequence_number, parent_chunk)
  - [ ] Add chunk quality metrics (character count, sentence count)
  - [ ] Store processing timestamp and version information

- [ ] Implement language detection (AC: 3)
  - [ ] Integrate language detection library for English/Thai identification
  - [ ] Add content routing based on detected language
  - [ ] Setup language-specific text preprocessing rules
  - [ ] Handle mixed-language content appropriately
  - [ ] Add language confidence scoring for chunk metadata

- [ ] Build content preprocessing pipeline (AC: 4)
  - [ ] Implement text cleaning (remove extra whitespace, special chars)
  - [ ] Add text normalization (Unicode, encoding standardization)
  - [ ] Create content quality validation rules
  - [ ] Filter out low-quality chunks (too short, repetitive, garbled)
  - [ ] Handle educational content patterns (formulas, equations, lists)

- [ ] Setup chunk storage system (AC: 5)
  - [ ] Implement ChunkRecord model with PostgreSQL storage
  - [ ] Create database indexes for efficient chunk retrieval
  - [ ] Setup relationships with textbooks and future topics tables
  - [ ] Add batch insertion for performance optimization
  - [ ] Implement chunk deduplication logic

- [ ] Configure async processing (AC: 6)
  - [ ] Create chunking_task for Celery background processing
  - [ ] Implement progress tracking for large document processing
  - [ ] Add error handling and retry mechanisms for failed chunks
  - [ ] Setup job result aggregation and reporting
  - [ ] Handle memory optimization for large documents

- [ ] Optimize processing performance (AC: 7)
  - [ ] Implement streaming processing for large PDFs
  - [ ] Add parallel processing for independent text sections
  - [ ] Optimize database batch operations
  - [ ] Setup processing metrics and monitoring
  - [ ] Handle resource cleanup and memory management

- [ ] Create integration tests (AC: 8)
  - [ ] Test chunking strategy with sample Physics textbooks
  - [ ] Validate chunk metadata accuracy and completeness
  - [ ] Test language detection with mixed-language content
  - [ ] Test chunk storage and retrieval operations
  - [ ] Test async processing with large documents

## Dev Notes

### Previous Story Insights
Builds on Story 2.1 content ingestion pipeline with textbook records and PDF processing capabilities already established.

### Data Models
**Chunk Storage Schema (Source: requirements.md FR4, technical-assumptions.md):**
- **chunks table**: Store processed content chunks with metadata and relationships
- **Fields**: chunk_id (hash), textbook_id, content, metadata, language, quality_score, created_at
- **Indexes**: chunk_id (primary), textbook_id (foreign key), language (filter index)
- **Relationships**: Links to textbooks table and future vector embeddings

### API Specifications
**Chunking Processing (Source: requirements.md FR5, FR13):**
- **Internal API**: Chunking service called by content processing pipeline
- **Async Processing**: Celery tasks for background chunking of large documents
- **Progress Tracking**: Job status updates for chunking completion monitoring
- **Configuration**: YAML-driven chunking parameters and text processing rules

### Component Specifications
**Chunking Service Architecture (Source: technical-assumptions.md):**
- **RecursiveCharacterTextSplitter**: Optimized for educational content chunking
- **Language Detection**: Basic English/Thai routing for content processing
- **Hash-based IDs**: Unique chunk identification for deduplication
- **Metadata Generation**: Comprehensive source tracking and quality metrics
- **Batch Processing**: Efficient database operations for chunk storage

### File Locations
**Chunking Components:**
- `/src/services/chunking_service.py`: ChunkProcessor and metadata generation
- `/src/models/chunk.py`: ChunkRecord model and database operations
- `/src/tasks/chunking_tasks.py`: Celery tasks for async chunking
- `/src/config/chunking.yaml`: Configurable chunking parameters
- `/tests/test_chunking.py`: Chunking quality and integration tests

### Testing Requirements
**Chunking Validation Tests (Source: requirements.md NFR14):**
- **Chunk Quality**: Validate chunk boundaries, size consistency, metadata accuracy
- **Language Detection**: Test with English, Thai, and mixed-language content
- **Performance**: Test processing speed with standard Physics textbooks
- **Storage Operations**: Test batch insertion, indexing, and retrieval performance
- **Error Handling**: Test with corrupted text, encoding issues, memory limitations

### Technical Constraints
**Chunking Requirements (Source: requirements.md FR3, FR5, technical-assumptions.md):**
- **Chunk Strategy**: Fixed-size chunks with overlap for basic context preservation
- **Chunk Size**: ~1000 characters with 200 character overlap for optimal retrieval
- **Language Support**: English-first with basic Thai support using bge-m3 compatibility
- **Storage**: PostgreSQL chunks table with proper indexing for ~50k chunks
- **Performance**: Process standard textbooks within reasonable timeframe for MVP

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-28 | v1.0 | Epic 2 content chunking story creation | Sarah (PO) |

## Dev Agent Record
*This section will be populated by the development agent during implementation*

## QA Results
*Results from QA Agent review will appear here after implementation*